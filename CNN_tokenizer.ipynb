{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantoshSaklani/Vulnerability-Severity/blob/main/CNN_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2vOjR2Jwr74A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtPwJP2fsEFh",
        "outputId": "e07544e3-d082-4064-88e8-6d39fc1dabaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "file_path = '/content/drive/My Drive/cvssV30_31_metricFinalDataset.csv'\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KunYlzIUsFWe"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anAAXsTWsFio",
        "outputId": "662a7243-0e9e-4afb-e1d3-b6a63c74323f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                       descriptions attackVector  \\\n",
            "0           0  ScriptAlias directory in NCSA and Apache httpd...      NETWORK   \n",
            "1           1  Cross-site scripting (XSS) vulnerability in th...      NETWORK   \n",
            "2           2  El protocolo DNS, como es implementado en (1) ...      NETWORK   \n",
            "3           3  El núcleo de Linux anterior a 2.6.25.10, no re...        LOCAL   \n",
            "4           4  La función do_change_type en fs/namespace.c de...        LOCAL   \n",
            "\n",
            "  attackComplexity privilegesRequired userInteraction      scope  \\\n",
            "0              LOW               NONE            NONE  UNCHANGED   \n",
            "1              LOW               HIGH        REQUIRED    CHANGED   \n",
            "2             HIGH               NONE            NONE    CHANGED   \n",
            "3              LOW                LOW            NONE  UNCHANGED   \n",
            "4              LOW                LOW            NONE  UNCHANGED   \n",
            "\n",
            "  confidentialityImpact integrityImpact availabilityImpact  baseScore  \\\n",
            "0                  HIGH            NONE               NONE        7.5   \n",
            "1                   LOW             LOW               NONE        4.8   \n",
            "2                  NONE            HIGH               NONE        6.8   \n",
            "3                  HIGH            HIGH               HIGH        7.8   \n",
            "4                  HIGH            HIGH               HIGH        7.8   \n",
            "\n",
            "  baseSeverity  exploitabilityScore  impactScore  \n",
            "0         HIGH                  3.9          3.6  \n",
            "1       MEDIUM                  1.7          2.7  \n",
            "2       MEDIUM                  2.2          4.0  \n",
            "3         HIGH                  1.8          5.9  \n",
            "4         HIGH                  1.8          5.9  \n"
          ]
        }
      ],
      "source": [
        "# Now you can work with the DataFrame 'df'\n",
        "print(df.head())  # Print the first few rows of the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRrrMv2xsFlp",
        "outputId": "a6a4e5bc-4bd8-4a45-c8be-257affa534ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after dropping unnamed column:\n",
            "                                             descriptions      attackVector  \\\n",
            "0       ScriptAlias directory in NCSA and Apache httpd...           NETWORK   \n",
            "1       Cross-site scripting (XSS) vulnerability in th...           NETWORK   \n",
            "2       El protocolo DNS, como es implementado en (1) ...           NETWORK   \n",
            "3       El núcleo de Linux anterior a 2.6.25.10, no re...             LOCAL   \n",
            "4       La función do_change_type en fs/namespace.c de...             LOCAL   \n",
            "...                                                   ...               ...   \n",
            "133423  A vulnerability classified as critical was fou...           NETWORK   \n",
            "133424  A vulnerability, which was classified as probl...  ADJACENT_NETWORK   \n",
            "133425  Desbordamiento de búfer de pila en el reposito...             LOCAL   \n",
            "133426  Cross-site Scripting (XSS) - Stored in GitHub ...           NETWORK   \n",
            "133427  Cross-site Scripting (XSS) - DOM in GitHub rep...           NETWORK   \n",
            "\n",
            "       attackComplexity privilegesRequired userInteraction      scope  \\\n",
            "0                   LOW               NONE            NONE  UNCHANGED   \n",
            "1                   LOW               HIGH        REQUIRED    CHANGED   \n",
            "2                  HIGH               NONE            NONE    CHANGED   \n",
            "3                   LOW                LOW            NONE  UNCHANGED   \n",
            "4                   LOW                LOW            NONE  UNCHANGED   \n",
            "...                 ...                ...             ...        ...   \n",
            "133423              LOW                LOW            NONE  UNCHANGED   \n",
            "133424              LOW                LOW            NONE  UNCHANGED   \n",
            "133425              LOW               NONE            NONE  UNCHANGED   \n",
            "133426              LOW               NONE        REQUIRED  UNCHANGED   \n",
            "133427              LOW               NONE        REQUIRED  UNCHANGED   \n",
            "\n",
            "       confidentialityImpact integrityImpact availabilityImpact  baseScore  \\\n",
            "0                       HIGH            NONE               NONE        7.5   \n",
            "1                        LOW             LOW               NONE        4.8   \n",
            "2                       NONE            HIGH               NONE        6.8   \n",
            "3                       HIGH            HIGH               HIGH        7.8   \n",
            "4                       HIGH            HIGH               HIGH        7.8   \n",
            "...                      ...             ...                ...        ...   \n",
            "133423                   LOW             LOW                LOW        6.3   \n",
            "133424                   LOW             LOW                LOW        5.5   \n",
            "133425                  NONE             LOW                LOW        5.1   \n",
            "133426                   LOW             LOW               HIGH        7.6   \n",
            "133427                  HIGH             LOW                LOW        7.6   \n",
            "\n",
            "       baseSeverity  exploitabilityScore  impactScore  \n",
            "0              HIGH                  3.9          3.6  \n",
            "1            MEDIUM                  1.7          2.7  \n",
            "2            MEDIUM                  2.2          4.0  \n",
            "3              HIGH                  1.8          5.9  \n",
            "4              HIGH                  1.8          5.9  \n",
            "...             ...                  ...          ...  \n",
            "133423       MEDIUM                  2.8          3.4  \n",
            "133424       MEDIUM                  2.1          3.4  \n",
            "133425       MEDIUM                  2.5          2.5  \n",
            "133426         HIGH                  2.8          4.7  \n",
            "133427         HIGH                  2.8          4.7  \n",
            "\n",
            "[133428 rows x 13 columns]\n"
          ]
        }
      ],
      "source": [
        "# Drop unnamed column\n",
        "df = df.drop(columns=df.columns[df.columns.str.contains('Unnamed:')])\n",
        "\n",
        "print(\"DataFrame after dropping unnamed column:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vJFjYQqsFoj",
        "outputId": "24508809-7faf-4400-932d-c378cd922cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for 'attackVector':\n",
            "NETWORK             96612\n",
            "LOCAL               31677\n",
            "ADJACENT_NETWORK     3646\n",
            "PHYSICAL             1493\n",
            "Name: attackVector, dtype: int64\n",
            "\n",
            "Counts for 'attackComplexity':\n",
            "LOW     120988\n",
            "HIGH     12440\n",
            "Name: attackComplexity, dtype: int64\n",
            "\n",
            "Counts for 'privilegesRequired':\n",
            "NONE    82870\n",
            "LOW     38947\n",
            "HIGH    11611\n",
            "Name: privilegesRequired, dtype: int64\n",
            "\n",
            "Counts for 'userInteraction':\n",
            "NONE        86449\n",
            "REQUIRED    46979\n",
            "Name: userInteraction, dtype: int64\n",
            "\n",
            "Counts for 'scope':\n",
            "UNCHANGED    108378\n",
            "CHANGED       25050\n",
            "Name: scope, dtype: int64\n",
            "\n",
            "Counts for 'confidentialityImpact':\n",
            "HIGH    74579\n",
            "NONE    30079\n",
            "LOW     28770\n",
            "Name: confidentialityImpact, dtype: int64\n",
            "\n",
            "Counts for 'integrityImpact':\n",
            "HIGH    64463\n",
            "NONE    41412\n",
            "LOW     27553\n",
            "Name: integrityImpact, dtype: int64\n",
            "\n",
            "Counts for 'availabilityImpact':\n",
            "HIGH    73893\n",
            "NONE    52075\n",
            "LOW      7460\n",
            "Name: availabilityImpact, dtype: int64\n",
            "\n",
            "Counts for 'baseScore':\n",
            "9.8    15861\n",
            "7.5    15340\n",
            "7.8    13372\n",
            "8.8    12037\n",
            "6.5     9125\n",
            "       ...  \n",
            "1.8       14\n",
            "2.1       14\n",
            "1.6        3\n",
            "1.7        1\n",
            "9.2        1\n",
            "Name: baseScore, Length: 84, dtype: int64\n",
            "\n",
            "Counts for 'baseSeverity':\n",
            "MEDIUM      55785\n",
            "HIGH        54214\n",
            "CRITICAL    19401\n",
            "LOW          4010\n",
            "NONE           18\n",
            "Name: baseSeverity, dtype: int64\n",
            "\n",
            "Counts for 'exploitabilityScore':\n",
            "3.9    37947\n",
            "2.8    36729\n",
            "1.8    23263\n",
            "2.3     6136\n",
            "2.2     4631\n",
            "1.2     4565\n",
            "1.6     2955\n",
            "0.8     2559\n",
            "1.7     2187\n",
            "2.1     2062\n",
            "1.0     1842\n",
            "0.9     1588\n",
            "2.5     1335\n",
            "3.1     1202\n",
            "1.3      924\n",
            "2.0      833\n",
            "1.5      574\n",
            "0.5      570\n",
            "0.7      477\n",
            "1.1      289\n",
            "1.4      267\n",
            "0.6      201\n",
            "0.3      156\n",
            "0.4       86\n",
            "0.2       34\n",
            "0.1       16\n",
            "Name: exploitabilityScore, dtype: int64\n",
            "\n",
            "Counts for 'impactScore':\n",
            "5.9    50546\n",
            "3.6    34832\n",
            "2.7    15916\n",
            "1.4    12172\n",
            "5.2     4088\n",
            "6.0     3720\n",
            "3.4     2628\n",
            "2.5     2421\n",
            "4.0     2250\n",
            "4.7     1379\n",
            "4.2     1303\n",
            "3.7      822\n",
            "5.8      583\n",
            "5.5      495\n",
            "5.3      255\n",
            "0.0       18\n",
            "Name: impactScore, dtype: int64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assuming your DataFrame is named new_df\n",
        "column_value_counts = {}\n",
        "for column in df.columns:\n",
        "    if column != 'descriptions':  # Exclude the 'descriptions' column\n",
        "        column_value_counts[column] = df[column].value_counts()\n",
        "\n",
        "# Print the counts for each column (excluding 'descriptions')\n",
        "for column, counts in column_value_counts.items():\n",
        "    print(f\"Counts for '{column}':\\n{counts}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfQULqFQsFrr",
        "outputId": "43c6664e-df3b-42c7-e484-9a28bbc5f808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imbalanced Classes:\n",
            "ADJACENT_NETWORK    2.732560\n",
            "PHYSICAL            1.118956\n",
            "Name: attackVector, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Load your DataFrame (replace this with your actual code to load data)\n",
        "# new_df2 = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Choose the metric you want to analyze (e.g., 'attackVector')\n",
        "selected_metric = 'attackVector'\n",
        "\n",
        "# Calculate class distribution\n",
        "class_counts = df[selected_metric].value_counts()\n",
        "\n",
        "# Calculate the percentage of samples in each class\n",
        "class_percentages = class_counts / class_counts.sum() * 100\n",
        "\n",
        "# Identify imbalanced classes based on a threshold (e.g., 10%)\n",
        "imbalanced_threshold = 10\n",
        "imbalanced_classes = class_percentages[class_percentages < imbalanced_threshold]\n",
        "\n",
        "# Print imbalanced classes\n",
        "print(\"Imbalanced Classes:\")\n",
        "print(imbalanced_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLUlM0ihsFua",
        "outputId": "2beea93c-6690-44bb-84d3-a2d0d611dbee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imbalanced Classes:\n",
            "HIGH    9.32338\n",
            "Name: attackComplexity, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Load your DataFrame (replace this with your actual code to load data)\n",
        "# new_df2 = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Choose the metric you want to analyze (e.g., 'attackVector')\n",
        "selected_metric = 'attackComplexity'\n",
        "\n",
        "# Calculate class distribution\n",
        "class_counts = df[selected_metric].value_counts()\n",
        "\n",
        "# Calculate the percentage of samples in each class\n",
        "class_percentages = class_counts / class_counts.sum() * 100\n",
        "\n",
        "# Identify imbalanced classes based on a threshold (e.g., 10%)\n",
        "imbalanced_threshold = 10\n",
        "imbalanced_classes = class_percentages[class_percentages < imbalanced_threshold]\n",
        "\n",
        "# Print imbalanced classes\n",
        "print(\"Imbalanced Classes:\")\n",
        "print(imbalanced_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrzzAs0AsFxk",
        "outputId": "7148dc74-b205-4f13-d3ea-dc788b8c3d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imbalanced Classes:\n",
            "LOW    5.59103\n",
            "Name: availabilityImpact, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Load your DataFrame (replace this with your actual code to load data)\n",
        "# new_df2 = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Choose the metric you want to analyze (e.g., 'attackVector')\n",
        "selected_metric = 'availabilityImpact'\n",
        "\n",
        "# Calculate class distribution\n",
        "class_counts = df[selected_metric].value_counts()\n",
        "\n",
        "# Calculate the percentage of samples in each class\n",
        "class_percentages = class_counts / class_counts.sum() * 100\n",
        "\n",
        "# Identify imbalanced classes based on a threshold (e.g., 10%)\n",
        "imbalanced_threshold = 10\n",
        "imbalanced_classes = class_percentages[class_percentages < imbalanced_threshold]\n",
        "\n",
        "# Print imbalanced classes\n",
        "print(\"Imbalanced Classes:\")\n",
        "print(imbalanced_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0sepadO6sF0k"
      },
      "outputs": [],
      "source": [
        "# List of imbalanced classes for each metric\n",
        "imbalanced_classes = {\n",
        "    'attackVector': ['ADJACENT_NETWORK','PHYSICAL'],\n",
        "    'availabilityImpact': ['LOW']\n",
        "}\n",
        "\n",
        "# Filter out rows containing imbalanced classes\n",
        "balanced_data = df[~df.isin(imbalanced_classes).any(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKUfVOqHsF36",
        "outputId": "76a930ee-f89f-4fea-f3f6-e845baed52f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique classes for attackVector: 2\n",
            "Class Counts for attackVector:\n",
            "NETWORK    91110\n",
            "LOCAL      30356\n",
            "Name: attackVector, dtype: int64\n",
            "Number of unique classes for attackComplexity: 2\n",
            "Class Counts for attackComplexity:\n",
            "LOW     110793\n",
            "HIGH     10673\n",
            "Name: attackComplexity, dtype: int64\n",
            "Number of unique classes for privilegesRequired: 3\n",
            "Class Counts for privilegesRequired:\n",
            "NONE    75963\n",
            "LOW     35350\n",
            "HIGH    10153\n",
            "Name: privilegesRequired, dtype: int64\n",
            "Number of unique classes for userInteraction: 2\n",
            "Class Counts for userInteraction:\n",
            "NONE        76897\n",
            "REQUIRED    44569\n",
            "Name: userInteraction, dtype: int64\n",
            "Number of unique classes for scope: 2\n",
            "Class Counts for scope:\n",
            "UNCHANGED    98629\n",
            "CHANGED      22837\n",
            "Name: scope, dtype: int64\n",
            "Number of unique classes for confidentialityImpact: 3\n",
            "Class Counts for confidentialityImpact:\n",
            "HIGH    70497\n",
            "NONE    26517\n",
            "LOW     24452\n",
            "Name: confidentialityImpact, dtype: int64\n",
            "Number of unique classes for integrityImpact: 3\n",
            "Class Counts for integrityImpact:\n",
            "HIGH    61218\n",
            "NONE    37506\n",
            "LOW     22742\n",
            "Name: integrityImpact, dtype: int64\n",
            "Number of unique classes for availabilityImpact: 2\n",
            "Class Counts for availabilityImpact:\n",
            "HIGH    70934\n",
            "NONE    50532\n",
            "Name: availabilityImpact, dtype: int64\n",
            "Number of unique classes for baseSeverity: 5\n",
            "Class Counts for baseSeverity:\n",
            "HIGH        51245\n",
            "MEDIUM      48194\n",
            "CRITICAL    19023\n",
            "LOW          2994\n",
            "NONE           10\n",
            "Name: baseSeverity, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# List of metrics for which you want to count unique classes\n",
        "metrics_to_count = ['attackVector','attackComplexity','privilegesRequired','userInteraction','scope','confidentialityImpact','integrityImpact','availabilityImpact','baseSeverity']\n",
        "\n",
        "# Count the number of unique classes for each metric and their occurrences\n",
        "for metric in metrics_to_count:\n",
        "    unique_classes =balanced_data[metric].nunique()\n",
        "    class_counts = balanced_data[metric].value_counts()\n",
        "\n",
        "    print(f\"Number of unique classes for {metric}: {unique_classes}\")\n",
        "    print(f\"Class Counts for {metric}:\\n{class_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QnJwULDtUbr",
        "outputId": "1b8a790a-e7b2-4260-c15d-c60bebe10440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique classes for attackVector: 2\n",
            "Class Counts for attackVector:\n",
            "NETWORK    27250\n",
            "LOCAL       9190\n",
            "Name: attackVector, dtype: int64\n",
            "Number of unique classes for attackComplexity: 2\n",
            "Class Counts for attackComplexity:\n",
            "LOW     33283\n",
            "HIGH     3157\n",
            "Name: attackComplexity, dtype: int64\n",
            "Number of unique classes for privilegesRequired: 3\n",
            "Class Counts for privilegesRequired:\n",
            "NONE    22806\n",
            "LOW     10574\n",
            "HIGH     3060\n",
            "Name: privilegesRequired, dtype: int64\n",
            "Number of unique classes for userInteraction: 2\n",
            "Class Counts for userInteraction:\n",
            "NONE        23078\n",
            "REQUIRED    13362\n",
            "Name: userInteraction, dtype: int64\n",
            "Number of unique classes for scope: 2\n",
            "Class Counts for scope:\n",
            "UNCHANGED    29708\n",
            "CHANGED       6732\n",
            "Name: scope, dtype: int64\n",
            "Number of unique classes for confidentialityImpact: 3\n",
            "Class Counts for confidentialityImpact:\n",
            "HIGH    21270\n",
            "NONE     7905\n",
            "LOW      7265\n",
            "Name: confidentialityImpact, dtype: int64\n",
            "Number of unique classes for integrityImpact: 3\n",
            "Class Counts for integrityImpact:\n",
            "HIGH    18497\n",
            "NONE    11176\n",
            "LOW      6767\n",
            "Name: integrityImpact, dtype: int64\n",
            "Number of unique classes for availabilityImpact: 2\n",
            "Class Counts for availabilityImpact:\n",
            "HIGH    21306\n",
            "NONE    15134\n",
            "Name: availabilityImpact, dtype: int64\n",
            "Number of unique classes for baseSeverity: 5\n",
            "Class Counts for baseSeverity:\n",
            "HIGH        15514\n",
            "MEDIUM      14365\n",
            "CRITICAL     5673\n",
            "LOW           884\n",
            "NONE            4\n",
            "Name: baseSeverity, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(balanced_data, test_size=0.3, random_state=42)\n",
        "\n",
        "# Count the number of unique classes for each metric and their occurrences\n",
        "for metric in metrics_to_count:\n",
        "    unique_classes =test_df[metric].nunique()\n",
        "    class_counts = test_df[metric].value_counts()\n",
        "\n",
        "    print(f\"Number of unique classes for {metric}: {unique_classes}\")\n",
        "    print(f\"Class Counts for {metric}:\\n{class_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG1xyn5tulht"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzGwsft2tUqP",
        "outputId": "f7e662e0-4b9d-455d-e83e-dfe333ee63ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values for attackVector: ['NETWORK' 'LOCAL']\n",
            "Unique values for attackComplexity: ['LOW' 'HIGH']\n",
            "Unique values for privilegesRequired: ['NONE' 'LOW' 'HIGH']\n",
            "Unique values for userInteraction: ['NONE' 'REQUIRED']\n",
            "Unique values for scope: ['UNCHANGED' 'CHANGED']\n",
            "Unique values for confidentialityImpact: ['HIGH' 'NONE' 'LOW']\n",
            "Unique values for integrityImpact: ['NONE' 'LOW' 'HIGH']\n",
            "Unique values for availabilityImpact: ['HIGH' 'NONE']\n",
            "Unique values for baseSeverity: ['CRITICAL' 'MEDIUM' 'HIGH' 'LOW' 'NONE']\n"
          ]
        }
      ],
      "source": [
        "metrics1 = ['attackVector','attackComplexity','privilegesRequired','userInteraction','scope','confidentialityImpact','integrityImpact','availabilityImpact','baseSeverity']\n",
        "# Print the unique values for each metric\n",
        "for metric in metrics1:\n",
        "    unique_values = train_df[metric].unique()\n",
        "    print(f\"Unique values for {metric}: {unique_values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PQDmyhMuoQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c8ea311-f216-4950-a516-0f91456a7c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1063/1063 [==============================] - 575s 540ms/step - loss: 0.2313 - accuracy: 0.9057 - val_loss: 0.1771 - val_accuracy: 0.9326\n",
            "Epoch 2/2\n",
            "1063/1063 [==============================] - 570s 536ms/step - loss: 0.1118 - accuracy: 0.9590 - val_loss: 0.1824 - val_accuracy: 0.9323\n",
            "Time taken: 1163.2096962928772 seconds\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import time\n",
        "metrics = [ 'attackVector' ]\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['descriptions'])\n",
        "X_train = tokenizer.texts_to_sequences(train_df['descriptions'])\n",
        "X_train = pad_sequences(X_train)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_df['descriptions'])\n",
        "X_test = pad_sequences(X_test, maxlen=X_train.shape[1])\n",
        "\n",
        "# Convert categorical labels to numerical using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_shape),\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train a CNN model for each metric\n",
        "models = {}\n",
        "for metric in metrics:\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    model = create_cnn_model(X_train.shape[1], num_classes)\n",
        "    y_train = tf.keras.utils.to_categorical(train_df[metric], num_classes=num_classes)\n",
        "    model.fit(X_train, y_train, epochs=2, batch_size= 64, validation_split=0.2)\n",
        "    models[metric] = model\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Save the trained models\n",
        "for metric, model in models.items():\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/CNN_tokenizer/{metric}_CNN-tokenizer.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhyrRseGAW2f",
        "outputId": "dbdfc950-4c13-4bd5-d2d3-1ab5079b1770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for metric 'attackVector' saved as '/content/drive/My Drive/trained_models/CNN_tokenizer/attackVector_CNN-tokenizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Make predictions using the trained model\n",
        "    predicted_probabilities = model.predict(X_test)\n",
        "\n",
        "    # Convert continuous probabilities to predicted labels\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predicted_labels)\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predicted_labels, output_dict=True)\n",
        "    precision = report['weighted avg']['precision']\n",
        "    recall = report['weighted avg']['recall']\n",
        "    f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFbIYodfAXGn",
        "outputId": "073f54b8-590c-484f-9075-dfdec1d2c18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1139/1139 [==============================] - 68s 60ms/step\n",
            "Performance metrics for attackVector:\n",
            "Accuracy: 0.9321899012074644\n",
            "Precision: 0.9317618831868204\n",
            "Recall: 0.9321899012074644\n",
            "F1-score: 0.9319396076128433\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import time\n",
        "metrics = [ 'attackComplexity' ]\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['descriptions'])\n",
        "X_train = tokenizer.texts_to_sequences(train_df['descriptions'])\n",
        "X_train = pad_sequences(X_train)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_df['descriptions'])\n",
        "X_test = pad_sequences(X_test, maxlen=X_train.shape[1])\n",
        "\n",
        "# Convert categorical labels to numerical using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_shape),\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train a CNN model for each metric\n",
        "models = {}\n",
        "for metric in metrics:\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    model = create_cnn_model(X_train.shape[1], num_classes)\n",
        "    y_train = tf.keras.utils.to_categorical(train_df[metric], num_classes=num_classes)\n",
        "    model.fit(X_train, y_train, epochs=2, batch_size= 64, validation_split=0.2)\n",
        "    models[metric] = model\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6LPPTnaAXJv",
        "outputId": "cb7b616d-34b0-42f5-e2aa-21a5bfc62d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1063/1063 [==============================] - 623s 585ms/step - loss: 0.2080 - accuracy: 0.9317 - val_loss: 0.1873 - val_accuracy: 0.9362\n",
            "Epoch 2/2\n",
            "1063/1063 [==============================] - 613s 576ms/step - loss: 0.1241 - accuracy: 0.9539 - val_loss: 0.2045 - val_accuracy: 0.9384\n",
            "Time taken: 1282.9305386543274 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Save the trained models\n",
        "for metric, model in models.items():\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/CNN_tokenizer/{metric}_CNN-tokenizer.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyLPCRCDAXNK",
        "outputId": "e91ce86a-04bc-44a2-da6d-be708e5026d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for metric 'attackComplexity' saved as '/content/drive/My Drive/trained_models/CNN_tokenizer/attackComplexity_CNN-tokenizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Make predictions using the trained model\n",
        "    predicted_probabilities = model.predict(X_test)\n",
        "\n",
        "    # Convert continuous probabilities to predicted labels\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predicted_labels)\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predicted_labels, output_dict=True)\n",
        "    precision = report['weighted avg']['precision']\n",
        "    recall = report['weighted avg']['recall']\n",
        "    f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKtPt7yOAXQM",
        "outputId": "8034eeb2-eea4-4514-dcc7-441dae4c6d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1139/1139 [==============================] - 69s 60ms/step\n",
            "Performance metrics for attackComplexity:\n",
            "Accuracy: 0.936827661909989\n",
            "Precision: 0.9290151491525829\n",
            "Recall: 0.936827661909989\n",
            "F1-score: 0.9285261449288845\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import time\n",
        "metrics = [ 'privilegesRequired' ]\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['descriptions'])\n",
        "X_train = tokenizer.texts_to_sequences(train_df['descriptions'])\n",
        "X_train = pad_sequences(X_train)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_df['descriptions'])\n",
        "X_test = pad_sequences(X_test, maxlen=X_train.shape[1])\n",
        "\n",
        "# Convert categorical labels to numerical using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_shape),\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train a CNN model for each metric\n",
        "models = {}\n",
        "for metric in metrics:\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    model = create_cnn_model(X_train.shape[1], num_classes)\n",
        "    y_train = tf.keras.utils.to_categorical(train_df[metric], num_classes=num_classes)\n",
        "    model.fit(X_train, y_train, epochs=1, batch_size= 64, validation_split=0.2)\n",
        "    models[metric] = model\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi6rbrbYAXTN",
        "outputId": "81cec316-ca23-4ead-adeb-15ed89ec58c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1063/1063 [==============================] - 637s 599ms/step - loss: 0.5136 - accuracy: 0.7990 - val_loss: 0.4320 - val_accuracy: 0.8324\n",
            "Time taken: 637.833557844162 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Save the trained models\n",
        "for metric, model in models.items():\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/CNN_tokenizer/{metric}_CNN-tokenizer.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH8e2nz3AXWQ",
        "outputId": "69a777fa-b550-4049-e190-6fc50f8f856a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for metric 'privilegesRequired' saved as '/content/drive/My Drive/trained_models/CNN_tokenizer/privilegesRequired_CNN-tokenizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Make predictions using the trained model\n",
        "    predicted_probabilities = model.predict(X_test)\n",
        "\n",
        "    # Convert continuous probabilities to predicted labels\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predicted_labels)\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predicted_labels, output_dict=True)\n",
        "    precision = report['weighted avg']['precision']\n",
        "    recall = report['weighted avg']['recall']\n",
        "    f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psa0WRKfAXZt",
        "outputId": "dbd933e0-0759-4465-d2f5-b7a8167a52a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1139/1139 [==============================] - 74s 65ms/step\n",
            "Performance metrics for privilegesRequired:\n",
            "Accuracy: 0.832436882546652\n",
            "Precision: 0.8298992441596716\n",
            "Recall: 0.832436882546652\n",
            "F1-score: 0.8261854874117026\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import time\n",
        "metrics = [ 'userInteraction' ]\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['descriptions'])\n",
        "X_train = tokenizer.texts_to_sequences(train_df['descriptions'])\n",
        "X_train = pad_sequences(X_train)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_df['descriptions'])\n",
        "X_test = pad_sequences(X_test, maxlen=X_train.shape[1])\n",
        "\n",
        "# Convert categorical labels to numerical using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_shape),\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train a CNN model for each metric\n",
        "models = {}\n",
        "for metric in metrics:\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    model = create_cnn_model(X_train.shape[1], num_classes)\n",
        "    y_train = tf.keras.utils.to_categorical(train_df[metric], num_classes=num_classes)\n",
        "    model.fit(X_train, y_train, epochs=1, batch_size= 64, validation_split=0.2)\n",
        "    models[metric] = model\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIiKYX5MPoq4",
        "outputId": "f22b9547-319b-4cd7-db94-f6827317e603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1063/1063 [==============================] - 621s 584ms/step - loss: 0.2543 - accuracy: 0.9024 - val_loss: 0.2190 - val_accuracy: 0.9206\n",
            "Time taken: 622.1466307640076 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Save the trained models\n",
        "for metric, model in models.items():\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/CNN_tokenizer/{metric}_CNN-tokenizer.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdkdUMC-Po3X",
        "outputId": "83c93058-e1be-40e0-d256-bbf2d9d77bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for metric 'userInteraction' saved as '/content/drive/My Drive/trained_models/CNN_tokenizer/userInteraction_CNN-tokenizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Make predictions using the trained model\n",
        "    predicted_probabilities = model.predict(X_test)\n",
        "\n",
        "    # Convert continuous probabilities to predicted labels\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predicted_labels)\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predicted_labels, output_dict=True)\n",
        "    precision = report['weighted avg']['precision']\n",
        "    recall = report['weighted avg']['recall']\n",
        "    f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLhqMOtkPpBT",
        "outputId": "5a45157b-09fe-4e6c-de91-3d7c39967b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1139/1139 [==============================] - 72s 64ms/step\n",
            "Performance metrics for userInteraction:\n",
            "Accuracy: 0.9203073545554336\n",
            "Precision: 0.9205453169168949\n",
            "Recall: 0.9203073545554336\n",
            "F1-score: 0.9204070995520744\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import time\n",
        "metrics = [ 'scope' ]\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['descriptions'])\n",
        "X_train = tokenizer.texts_to_sequences(train_df['descriptions'])\n",
        "X_train = pad_sequences(X_train)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_df['descriptions'])\n",
        "X_test = pad_sequences(X_test, maxlen=X_train.shape[1])\n",
        "\n",
        "# Convert categorical labels to numerical using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_shape),\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train a CNN model for each metric\n",
        "models = {}\n",
        "for metric in metrics:\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    model = create_cnn_model(X_train.shape[1], num_classes)\n",
        "    y_train = tf.keras.utils.to_categorical(train_df[metric], num_classes=num_classes)\n",
        "    model.fit(X_train, y_train, epochs=1, batch_size= 64, validation_split=0.2)\n",
        "    models[metric] = model\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3scfQ9xPpNy",
        "outputId": "e33ba255-ee4a-42c1-ff69-1eafc2d83809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1063/1063 [==============================] - 615s 578ms/step - loss: 0.2046 - accuracy: 0.9284 - val_loss: 0.1716 - val_accuracy: 0.9382\n",
            "Time taken: 615.6322677135468 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Save the trained models\n",
        "for metric, model in models.items():\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/CNN_tokenizer/{metric}_CNN-tokenizer.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd8pHc9OPpRJ",
        "outputId": "e50c5041-4db9-4877-b472-c0e3da4361e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for metric 'scope' saved as '/content/drive/My Drive/trained_models/CNN_tokenizer/scope_CNN-tokenizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Make predictions using the trained model\n",
        "    predicted_probabilities = model.predict(X_test)\n",
        "\n",
        "    # Convert continuous probabilities to predicted labels\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predicted_labels)\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predicted_labels, output_dict=True)\n",
        "    precision = report['weighted avg']['precision']\n",
        "    recall = report['weighted avg']['recall']\n",
        "    f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6nrxU7NPpUf",
        "outputId": "51e3f598-c7e7-4c62-9d37-01748387a7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1139/1139 [==============================] - 69s 60ms/step\n",
            "Performance metrics for scope:\n",
            "Accuracy: 0.9395993413830955\n",
            "Precision: 0.9382003511121053\n",
            "Recall: 0.9395993413830955\n",
            "F1-score: 0.9375758133547469\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import time\n",
        "metrics = [ 'confidentialityImpact' ]\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['descriptions'])\n",
        "X_train = tokenizer.texts_to_sequences(train_df['descriptions'])\n",
        "X_train = pad_sequences(X_train)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_df['descriptions'])\n",
        "X_test = pad_sequences(X_test, maxlen=X_train.shape[1])\n",
        "\n",
        "# Convert categorical labels to numerical using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_shape),\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train a CNN model for each metric\n",
        "models = {}\n",
        "for metric in metrics:\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    model = create_cnn_model(X_train.shape[1], num_classes)\n",
        "    y_train = tf.keras.utils.to_categorical(train_df[metric], num_classes=num_classes)\n",
        "    model.fit(X_train, y_train, epochs=1, batch_size= 64, validation_split=0.2)\n",
        "    models[metric] = model\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ],
      "metadata": {
        "id": "XYxFxPhNPpXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c98c26-ae04-412c-e5ed-8e2378b8e620"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1063/1063 [==============================] - 951s 892ms/step - loss: 0.4508 - accuracy: 0.8246 - val_loss: 0.3722 - val_accuracy: 0.8527\n",
            "Time taken: 952.7145195007324 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Save the trained models\n",
        "for metric, model in models.items():\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/CNN_tokenizer/{metric}_CNN-tokenizer.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")"
      ],
      "metadata": {
        "id": "gD1ubWMEPpZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "134a543f-17bb-452c-aa01-25f8feddeb50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for metric 'confidentialityImpact' saved as '/content/drive/My Drive/trained_models/CNN_tokenizer/confidentialityImpact_CNN-tokenizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Make predictions using the trained model\n",
        "    predicted_probabilities = model.predict(X_test)\n",
        "\n",
        "    # Convert continuous probabilities to predicted labels\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predicted_labels)\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predicted_labels, output_dict=True)\n",
        "    precision = report['weighted avg']['precision']\n",
        "    recall = report['weighted avg']['recall']\n",
        "    f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "mVX5wHQSPpdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a73dd9-3634-4674-f27c-6f264a30d47b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1139/1139 [==============================] - 99s 87ms/step\n",
            "Performance metrics for confidentialityImpact:\n",
            "Accuracy: 0.8560922063666301\n",
            "Precision: 0.8558833940932876\n",
            "Recall: 0.8560922063666301\n",
            "F1-score: 0.853404568849478\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import time\n",
        "metrics = [ 'integrityImpact' ]\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['descriptions'])\n",
        "X_train = tokenizer.texts_to_sequences(train_df['descriptions'])\n",
        "X_train = pad_sequences(X_train)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_df['descriptions'])\n",
        "X_test = pad_sequences(X_test, maxlen=X_train.shape[1])\n",
        "\n",
        "# Convert categorical labels to numerical using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_shape),\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train a CNN model for each metric\n",
        "models = {}\n",
        "for metric in metrics:\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    model = create_cnn_model(X_train.shape[1], num_classes)\n",
        "    y_train = tf.keras.utils.to_categorical(train_df[metric], num_classes=num_classes)\n",
        "    model.fit(X_train, y_train, epochs=1, batch_size= 64, validation_split=0.2)\n",
        "    models[metric] = model\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ],
      "metadata": {
        "id": "hUOaoSiPPpfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e265c7-af08-4ae9-e219-ace867cc5ca1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1063/1063 [==============================] - 908s 854ms/step - loss: 0.4291 - accuracy: 0.8337 - val_loss: 0.3539 - val_accuracy: 0.8678\n",
            "Time taken: 923.7273969650269 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Save the trained models\n",
        "for metric, model in models.items():\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/CNN_tokenizer/{metric}_CNN-tokenizer.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")"
      ],
      "metadata": {
        "id": "NtL1sS7QPpjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a637794-a7c4-4d28-a82d-c882f6cc34a9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for metric 'integrityImpact' saved as '/content/drive/My Drive/trained_models/CNN_tokenizer/integrityImpact_CNN-tokenizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Make predictions using the trained model\n",
        "    predicted_probabilities = model.predict(X_test)\n",
        "\n",
        "    # Convert continuous probabilities to predicted labels\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predicted_labels)\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predicted_labels, output_dict=True)\n",
        "    precision = report['weighted avg']['precision']\n",
        "    recall = report['weighted avg']['recall']\n",
        "    f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "UYKuyVfiPplz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d505b2af-b09b-43c7-b377-37f009918020"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1139/1139 [==============================] - 102s 90ms/step\n",
            "Performance metrics for integrityImpact:\n",
            "Accuracy: 0.8702250274423711\n",
            "Precision: 0.8705614507138763\n",
            "Recall: 0.8702250274423711\n",
            "F1-score: 0.8701356433439312\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import time\n",
        "metrics = [ 'availabilityImpact' ]\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Preprocess the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['descriptions'])\n",
        "X_train = tokenizer.texts_to_sequences(train_df['descriptions'])\n",
        "X_train = pad_sequences(X_train)\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(test_df['descriptions'])\n",
        "X_test = pad_sequences(X_test, maxlen=X_train.shape[1])\n",
        "\n",
        "# Convert categorical labels to numerical using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_shape),\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train a CNN model for each metric\n",
        "models = {}\n",
        "for metric in metrics:\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    model = create_cnn_model(X_train.shape[1], num_classes)\n",
        "    y_train = tf.keras.utils.to_categorical(train_df[metric], num_classes=num_classes)\n",
        "    model.fit(X_train, y_train, epochs=1, batch_size= 64, validation_split=0.2)\n",
        "    models[metric] = model\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ],
      "metadata": {
        "id": "OWkxDlMTPppJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4cbbee9-f493-4cb3-b0c0-4c5e40658d0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1063/1063 [==============================] - 932s 876ms/step - loss: 0.2752 - accuracy: 0.8825 - val_loss: 0.2305 - val_accuracy: 0.9053\n",
            "Time taken: 932.6243348121643 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "# Save the trained models\n",
        "for metric, model in models.items():\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/CNN_tokenizer/{metric}_CNN-tokenizer.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")"
      ],
      "metadata": {
        "id": "-Svjh_deT8TH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b14259-eabf-49b3-de6d-af82f767971e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for metric 'availabilityImpact' saved as '/content/drive/My Drive/trained_models/CNN_tokenizer/availabilityImpact_CNN-tokenizer.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Make predictions using the trained model\n",
        "    predicted_probabilities = model.predict(X_test)\n",
        "\n",
        "    # Convert continuous probabilities to predicted labels\n",
        "    predicted_labels = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predicted_labels)\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predicted_labels, output_dict=True)\n",
        "    precision = report['weighted avg']['precision']\n",
        "    recall = report['weighted avg']['recall']\n",
        "    f1_score = report['weighted avg']['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PSuaz3p_xsV",
        "outputId": "c76d582b-2663-48bc-db08-7b8b7444cc7c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1139/1139 [==============================] - 113s 99ms/step\n",
            "Performance metrics for availabilityImpact:\n",
            "Accuracy: 0.9039517014270033\n",
            "Precision: 0.9037834142849911\n",
            "Recall: 0.9039517014270033\n",
            "F1-score: 0.9037530668520893\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcTx8qXoOHtPpwHgPNBaMo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}