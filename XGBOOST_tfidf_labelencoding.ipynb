{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SantoshSaklani/Vulnerability-Severity/blob/main/XGBOOST_tfidf_labelencoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2vOjR2Jwr74A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtPwJP2fsEFh",
        "outputId": "6f36ee87-76c6-4ebc-a084-f0595f323c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "file_path = '/content/drive/My Drive/cvssV30_31_metricFinalDataset.csv'\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KunYlzIUsFWe"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anAAXsTWsFio",
        "outputId": "f2c0ef40-890c-43c0-96a5-0734db07e130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                       descriptions attackVector  \\\n",
            "0           0  ScriptAlias directory in NCSA and Apache httpd...      NETWORK   \n",
            "1           1  Cross-site scripting (XSS) vulnerability in th...      NETWORK   \n",
            "2           2  El protocolo DNS, como es implementado en (1) ...      NETWORK   \n",
            "3           3  El núcleo de Linux anterior a 2.6.25.10, no re...        LOCAL   \n",
            "4           4  La función do_change_type en fs/namespace.c de...        LOCAL   \n",
            "\n",
            "  attackComplexity privilegesRequired userInteraction      scope  \\\n",
            "0              LOW               NONE            NONE  UNCHANGED   \n",
            "1              LOW               HIGH        REQUIRED    CHANGED   \n",
            "2             HIGH               NONE            NONE    CHANGED   \n",
            "3              LOW                LOW            NONE  UNCHANGED   \n",
            "4              LOW                LOW            NONE  UNCHANGED   \n",
            "\n",
            "  confidentialityImpact integrityImpact availabilityImpact  baseScore  \\\n",
            "0                  HIGH            NONE               NONE        7.5   \n",
            "1                   LOW             LOW               NONE        4.8   \n",
            "2                  NONE            HIGH               NONE        6.8   \n",
            "3                  HIGH            HIGH               HIGH        7.8   \n",
            "4                  HIGH            HIGH               HIGH        7.8   \n",
            "\n",
            "  baseSeverity  exploitabilityScore  impactScore  \n",
            "0         HIGH                  3.9          3.6  \n",
            "1       MEDIUM                  1.7          2.7  \n",
            "2       MEDIUM                  2.2          4.0  \n",
            "3         HIGH                  1.8          5.9  \n",
            "4         HIGH                  1.8          5.9  \n"
          ]
        }
      ],
      "source": [
        "# Now you can work with the DataFrame 'df'\n",
        "print(df.head())  # Print the first few rows of the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRrrMv2xsFlp",
        "outputId": "cd893d4b-bf8e-4024-8cfc-f85bb70c4a64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after dropping unnamed column:\n",
            "                                             descriptions      attackVector  \\\n",
            "0       ScriptAlias directory in NCSA and Apache httpd...           NETWORK   \n",
            "1       Cross-site scripting (XSS) vulnerability in th...           NETWORK   \n",
            "2       El protocolo DNS, como es implementado en (1) ...           NETWORK   \n",
            "3       El núcleo de Linux anterior a 2.6.25.10, no re...             LOCAL   \n",
            "4       La función do_change_type en fs/namespace.c de...             LOCAL   \n",
            "...                                                   ...               ...   \n",
            "133423  A vulnerability classified as critical was fou...           NETWORK   \n",
            "133424  A vulnerability, which was classified as probl...  ADJACENT_NETWORK   \n",
            "133425  Desbordamiento de búfer de pila en el reposito...             LOCAL   \n",
            "133426  Cross-site Scripting (XSS) - Stored in GitHub ...           NETWORK   \n",
            "133427  Cross-site Scripting (XSS) - DOM in GitHub rep...           NETWORK   \n",
            "\n",
            "       attackComplexity privilegesRequired userInteraction      scope  \\\n",
            "0                   LOW               NONE            NONE  UNCHANGED   \n",
            "1                   LOW               HIGH        REQUIRED    CHANGED   \n",
            "2                  HIGH               NONE            NONE    CHANGED   \n",
            "3                   LOW                LOW            NONE  UNCHANGED   \n",
            "4                   LOW                LOW            NONE  UNCHANGED   \n",
            "...                 ...                ...             ...        ...   \n",
            "133423              LOW                LOW            NONE  UNCHANGED   \n",
            "133424              LOW                LOW            NONE  UNCHANGED   \n",
            "133425              LOW               NONE            NONE  UNCHANGED   \n",
            "133426              LOW               NONE        REQUIRED  UNCHANGED   \n",
            "133427              LOW               NONE        REQUIRED  UNCHANGED   \n",
            "\n",
            "       confidentialityImpact integrityImpact availabilityImpact  baseScore  \\\n",
            "0                       HIGH            NONE               NONE        7.5   \n",
            "1                        LOW             LOW               NONE        4.8   \n",
            "2                       NONE            HIGH               NONE        6.8   \n",
            "3                       HIGH            HIGH               HIGH        7.8   \n",
            "4                       HIGH            HIGH               HIGH        7.8   \n",
            "...                      ...             ...                ...        ...   \n",
            "133423                   LOW             LOW                LOW        6.3   \n",
            "133424                   LOW             LOW                LOW        5.5   \n",
            "133425                  NONE             LOW                LOW        5.1   \n",
            "133426                   LOW             LOW               HIGH        7.6   \n",
            "133427                  HIGH             LOW                LOW        7.6   \n",
            "\n",
            "       baseSeverity  exploitabilityScore  impactScore  \n",
            "0              HIGH                  3.9          3.6  \n",
            "1            MEDIUM                  1.7          2.7  \n",
            "2            MEDIUM                  2.2          4.0  \n",
            "3              HIGH                  1.8          5.9  \n",
            "4              HIGH                  1.8          5.9  \n",
            "...             ...                  ...          ...  \n",
            "133423       MEDIUM                  2.8          3.4  \n",
            "133424       MEDIUM                  2.1          3.4  \n",
            "133425       MEDIUM                  2.5          2.5  \n",
            "133426         HIGH                  2.8          4.7  \n",
            "133427         HIGH                  2.8          4.7  \n",
            "\n",
            "[133428 rows x 13 columns]\n"
          ]
        }
      ],
      "source": [
        "# Drop unnamed column\n",
        "df = df.drop(columns=df.columns[df.columns.str.contains('Unnamed:')])\n",
        "\n",
        "print(\"DataFrame after dropping unnamed column:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vJFjYQqsFoj",
        "outputId": "e3d8c6a4-feed-47b4-9a7c-900456b90677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for 'attackVector':\n",
            "NETWORK             96612\n",
            "LOCAL               31677\n",
            "ADJACENT_NETWORK     3646\n",
            "PHYSICAL             1493\n",
            "Name: attackVector, dtype: int64\n",
            "\n",
            "Counts for 'attackComplexity':\n",
            "LOW     120988\n",
            "HIGH     12440\n",
            "Name: attackComplexity, dtype: int64\n",
            "\n",
            "Counts for 'privilegesRequired':\n",
            "NONE    82870\n",
            "LOW     38947\n",
            "HIGH    11611\n",
            "Name: privilegesRequired, dtype: int64\n",
            "\n",
            "Counts for 'userInteraction':\n",
            "NONE        86449\n",
            "REQUIRED    46979\n",
            "Name: userInteraction, dtype: int64\n",
            "\n",
            "Counts for 'scope':\n",
            "UNCHANGED    108378\n",
            "CHANGED       25050\n",
            "Name: scope, dtype: int64\n",
            "\n",
            "Counts for 'confidentialityImpact':\n",
            "HIGH    74579\n",
            "NONE    30079\n",
            "LOW     28770\n",
            "Name: confidentialityImpact, dtype: int64\n",
            "\n",
            "Counts for 'integrityImpact':\n",
            "HIGH    64463\n",
            "NONE    41412\n",
            "LOW     27553\n",
            "Name: integrityImpact, dtype: int64\n",
            "\n",
            "Counts for 'availabilityImpact':\n",
            "HIGH    73893\n",
            "NONE    52075\n",
            "LOW      7460\n",
            "Name: availabilityImpact, dtype: int64\n",
            "\n",
            "Counts for 'baseScore':\n",
            "9.8    15861\n",
            "7.5    15340\n",
            "7.8    13372\n",
            "8.8    12037\n",
            "6.5     9125\n",
            "       ...  \n",
            "1.8       14\n",
            "2.1       14\n",
            "1.6        3\n",
            "1.7        1\n",
            "9.2        1\n",
            "Name: baseScore, Length: 84, dtype: int64\n",
            "\n",
            "Counts for 'baseSeverity':\n",
            "MEDIUM      55785\n",
            "HIGH        54214\n",
            "CRITICAL    19401\n",
            "LOW          4010\n",
            "NONE           18\n",
            "Name: baseSeverity, dtype: int64\n",
            "\n",
            "Counts for 'exploitabilityScore':\n",
            "3.9    37947\n",
            "2.8    36729\n",
            "1.8    23263\n",
            "2.3     6136\n",
            "2.2     4631\n",
            "1.2     4565\n",
            "1.6     2955\n",
            "0.8     2559\n",
            "1.7     2187\n",
            "2.1     2062\n",
            "1.0     1842\n",
            "0.9     1588\n",
            "2.5     1335\n",
            "3.1     1202\n",
            "1.3      924\n",
            "2.0      833\n",
            "1.5      574\n",
            "0.5      570\n",
            "0.7      477\n",
            "1.1      289\n",
            "1.4      267\n",
            "0.6      201\n",
            "0.3      156\n",
            "0.4       86\n",
            "0.2       34\n",
            "0.1       16\n",
            "Name: exploitabilityScore, dtype: int64\n",
            "\n",
            "Counts for 'impactScore':\n",
            "5.9    50546\n",
            "3.6    34832\n",
            "2.7    15916\n",
            "1.4    12172\n",
            "5.2     4088\n",
            "6.0     3720\n",
            "3.4     2628\n",
            "2.5     2421\n",
            "4.0     2250\n",
            "4.7     1379\n",
            "4.2     1303\n",
            "3.7      822\n",
            "5.8      583\n",
            "5.5      495\n",
            "5.3      255\n",
            "0.0       18\n",
            "Name: impactScore, dtype: int64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assuming your DataFrame is named new_df\n",
        "column_value_counts = {}\n",
        "for column in df.columns:\n",
        "    if column != 'descriptions':  # Exclude the 'descriptions' column\n",
        "        column_value_counts[column] = df[column].value_counts()\n",
        "\n",
        "# Print the counts for each column (excluding 'descriptions')\n",
        "for column, counts in column_value_counts.items():\n",
        "    print(f\"Counts for '{column}':\\n{counts}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfQULqFQsFrr",
        "outputId": "5cf78e7c-08c2-444e-b5d1-aec5d9bebfbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imbalanced Classes:\n",
            "ADJACENT_NETWORK    2.732560\n",
            "PHYSICAL            1.118956\n",
            "Name: attackVector, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Load your DataFrame (replace this with your actual code to load data)\n",
        "# new_df2 = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Choose the metric you want to analyze (e.g., 'attackVector')\n",
        "selected_metric = 'attackVector'\n",
        "\n",
        "# Calculate class distribution\n",
        "class_counts = df[selected_metric].value_counts()\n",
        "\n",
        "# Calculate the percentage of samples in each class\n",
        "class_percentages = class_counts / class_counts.sum() * 100\n",
        "\n",
        "# Identify imbalanced classes based on a threshold (e.g., 10%)\n",
        "imbalanced_threshold = 10\n",
        "imbalanced_classes = class_percentages[class_percentages < imbalanced_threshold]\n",
        "\n",
        "# Print imbalanced classes\n",
        "print(\"Imbalanced Classes:\")\n",
        "print(imbalanced_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLUlM0ihsFua",
        "outputId": "e5df158b-fa41-406a-fc9f-a3600e149703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imbalanced Classes:\n",
            "HIGH    9.32338\n",
            "Name: attackComplexity, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Load your DataFrame (replace this with your actual code to load data)\n",
        "# new_df2 = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Choose the metric you want to analyze (e.g., 'attackVector')\n",
        "selected_metric = 'attackComplexity'\n",
        "\n",
        "# Calculate class distribution\n",
        "class_counts = df[selected_metric].value_counts()\n",
        "\n",
        "# Calculate the percentage of samples in each class\n",
        "class_percentages = class_counts / class_counts.sum() * 100\n",
        "\n",
        "# Identify imbalanced classes based on a threshold (e.g., 10%)\n",
        "imbalanced_threshold = 10\n",
        "imbalanced_classes = class_percentages[class_percentages < imbalanced_threshold]\n",
        "\n",
        "# Print imbalanced classes\n",
        "print(\"Imbalanced Classes:\")\n",
        "print(imbalanced_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrzzAs0AsFxk",
        "outputId": "8ef8f2c7-2e70-4071-e869-a1bd1c7dd923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imbalanced Classes:\n",
            "LOW    5.59103\n",
            "Name: availabilityImpact, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Load your DataFrame (replace this with your actual code to load data)\n",
        "# new_df2 = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Choose the metric you want to analyze (e.g., 'attackVector')\n",
        "selected_metric = 'availabilityImpact'\n",
        "\n",
        "# Calculate class distribution\n",
        "class_counts = df[selected_metric].value_counts()\n",
        "\n",
        "# Calculate the percentage of samples in each class\n",
        "class_percentages = class_counts / class_counts.sum() * 100\n",
        "\n",
        "# Identify imbalanced classes based on a threshold (e.g., 10%)\n",
        "imbalanced_threshold = 10\n",
        "imbalanced_classes = class_percentages[class_percentages < imbalanced_threshold]\n",
        "\n",
        "# Print imbalanced classes\n",
        "print(\"Imbalanced Classes:\")\n",
        "print(imbalanced_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0sepadO6sF0k"
      },
      "outputs": [],
      "source": [
        "# List of imbalanced classes for each metric\n",
        "imbalanced_classes = {\n",
        "    'attackVector': ['ADJACENT_NETWORK','PHYSICAL'],\n",
        "    'availabilityImpact': ['LOW']\n",
        "}\n",
        "\n",
        "# Filter out rows containing imbalanced classes\n",
        "balanced_data = df[~df.isin(imbalanced_classes).any(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKUfVOqHsF36",
        "outputId": "21e1ad0f-b99e-4e80-c0d9-56063e3415d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique classes for attackVector: 2\n",
            "Class Counts for attackVector:\n",
            "NETWORK    91110\n",
            "LOCAL      30356\n",
            "Name: attackVector, dtype: int64\n",
            "Number of unique classes for attackComplexity: 2\n",
            "Class Counts for attackComplexity:\n",
            "LOW     110793\n",
            "HIGH     10673\n",
            "Name: attackComplexity, dtype: int64\n",
            "Number of unique classes for privilegesRequired: 3\n",
            "Class Counts for privilegesRequired:\n",
            "NONE    75963\n",
            "LOW     35350\n",
            "HIGH    10153\n",
            "Name: privilegesRequired, dtype: int64\n",
            "Number of unique classes for userInteraction: 2\n",
            "Class Counts for userInteraction:\n",
            "NONE        76897\n",
            "REQUIRED    44569\n",
            "Name: userInteraction, dtype: int64\n",
            "Number of unique classes for scope: 2\n",
            "Class Counts for scope:\n",
            "UNCHANGED    98629\n",
            "CHANGED      22837\n",
            "Name: scope, dtype: int64\n",
            "Number of unique classes for confidentialityImpact: 3\n",
            "Class Counts for confidentialityImpact:\n",
            "HIGH    70497\n",
            "NONE    26517\n",
            "LOW     24452\n",
            "Name: confidentialityImpact, dtype: int64\n",
            "Number of unique classes for integrityImpact: 3\n",
            "Class Counts for integrityImpact:\n",
            "HIGH    61218\n",
            "NONE    37506\n",
            "LOW     22742\n",
            "Name: integrityImpact, dtype: int64\n",
            "Number of unique classes for availabilityImpact: 2\n",
            "Class Counts for availabilityImpact:\n",
            "HIGH    70934\n",
            "NONE    50532\n",
            "Name: availabilityImpact, dtype: int64\n",
            "Number of unique classes for baseSeverity: 5\n",
            "Class Counts for baseSeverity:\n",
            "HIGH        51245\n",
            "MEDIUM      48194\n",
            "CRITICAL    19023\n",
            "LOW          2994\n",
            "NONE           10\n",
            "Name: baseSeverity, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# List of metrics for which you want to count unique classes\n",
        "metrics_to_count = ['attackVector','attackComplexity','privilegesRequired','userInteraction','scope','confidentialityImpact','integrityImpact','availabilityImpact','baseSeverity']\n",
        "\n",
        "# Count the number of unique classes for each metric and their occurrences\n",
        "for metric in metrics_to_count:\n",
        "    unique_classes =balanced_data[metric].nunique()\n",
        "    class_counts = balanced_data[metric].value_counts()\n",
        "\n",
        "    print(f\"Number of unique classes for {metric}: {unique_classes}\")\n",
        "    print(f\"Class Counts for {metric}:\\n{class_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QnJwULDtUbr",
        "outputId": "711b741b-0469-4a98-e862-aaadba87de84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique classes for attackVector: 2\n",
            "Class Counts for attackVector:\n",
            "NETWORK    27250\n",
            "LOCAL       9190\n",
            "Name: attackVector, dtype: int64\n",
            "Number of unique classes for attackComplexity: 2\n",
            "Class Counts for attackComplexity:\n",
            "LOW     33283\n",
            "HIGH     3157\n",
            "Name: attackComplexity, dtype: int64\n",
            "Number of unique classes for privilegesRequired: 3\n",
            "Class Counts for privilegesRequired:\n",
            "NONE    22806\n",
            "LOW     10574\n",
            "HIGH     3060\n",
            "Name: privilegesRequired, dtype: int64\n",
            "Number of unique classes for userInteraction: 2\n",
            "Class Counts for userInteraction:\n",
            "NONE        23078\n",
            "REQUIRED    13362\n",
            "Name: userInteraction, dtype: int64\n",
            "Number of unique classes for scope: 2\n",
            "Class Counts for scope:\n",
            "UNCHANGED    29708\n",
            "CHANGED       6732\n",
            "Name: scope, dtype: int64\n",
            "Number of unique classes for confidentialityImpact: 3\n",
            "Class Counts for confidentialityImpact:\n",
            "HIGH    21270\n",
            "NONE     7905\n",
            "LOW      7265\n",
            "Name: confidentialityImpact, dtype: int64\n",
            "Number of unique classes for integrityImpact: 3\n",
            "Class Counts for integrityImpact:\n",
            "HIGH    18497\n",
            "NONE    11176\n",
            "LOW      6767\n",
            "Name: integrityImpact, dtype: int64\n",
            "Number of unique classes for availabilityImpact: 2\n",
            "Class Counts for availabilityImpact:\n",
            "HIGH    21306\n",
            "NONE    15134\n",
            "Name: availabilityImpact, dtype: int64\n",
            "Number of unique classes for baseSeverity: 5\n",
            "Class Counts for baseSeverity:\n",
            "HIGH        15514\n",
            "MEDIUM      14365\n",
            "CRITICAL     5673\n",
            "LOW           884\n",
            "NONE            4\n",
            "Name: baseSeverity, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(balanced_data, test_size=0.3, random_state=42)\n",
        "\n",
        "# Count the number of unique classes for each metric and their occurrences\n",
        "for metric in metrics_to_count:\n",
        "    unique_classes =test_df[metric].nunique()\n",
        "    class_counts = test_df[metric].value_counts()\n",
        "\n",
        "    print(f\"Number of unique classes for {metric}: {unique_classes}\")\n",
        "    print(f\"Class Counts for {metric}:\\n{class_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG1xyn5tulht"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzGwsft2tUqP",
        "outputId": "14c8cf82-8bc4-4832-9205-134e15edd68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values for attackVector: ['NETWORK' 'LOCAL']\n",
            "Unique values for attackComplexity: ['LOW' 'HIGH']\n",
            "Unique values for privilegesRequired: ['NONE' 'LOW' 'HIGH']\n",
            "Unique values for userInteraction: ['NONE' 'REQUIRED']\n",
            "Unique values for scope: ['UNCHANGED' 'CHANGED']\n",
            "Unique values for confidentialityImpact: ['HIGH' 'NONE' 'LOW']\n",
            "Unique values for integrityImpact: ['NONE' 'LOW' 'HIGH']\n",
            "Unique values for availabilityImpact: ['HIGH' 'NONE']\n",
            "Unique values for baseSeverity: ['CRITICAL' 'MEDIUM' 'HIGH' 'LOW' 'NONE']\n"
          ]
        }
      ],
      "source": [
        "metrics1 = ['attackVector','attackComplexity','privilegesRequired','userInteraction','scope','confidentialityImpact','integrityImpact','availabilityImpact','baseSeverity']\n",
        "# Print the unique values for each metric\n",
        "for metric in metrics1:\n",
        "    unique_values = train_df[metric].unique()\n",
        "    print(f\"Unique values for {metric}: {unique_values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6PQDmyhMuoQy"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Initialize the vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Vectorize the vulnerability description\n",
        "X_train = vectorizer.fit_transform(train_df['descriptions'])\n",
        "X_test = vectorizer.transform(test_df['descriptions'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVG2XC2LxvuC",
        "outputId": "0d05bb2c-e7f1-43d7-a7b0-3f12f2d964a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model for metric 'attackVector' saved as '/content/drive/My Drive/trained_models/XGBOOST/attackVector_XGBoost_tfidf_labelEncoding.pkl'\n",
            "Time taken: 297.5498616695404 seconds\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics = ['attackVector']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Define the dictionary to store XGBoost models\n",
        "models = {}\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through each metric and train an XGBoost model without SMOTE\n",
        "for metric in metrics:\n",
        "    # Create the XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=0.1,    # Learning rate for boosting\n",
        "        n_estimators=500,     # Number of boosting rounds\n",
        "        max_depth=5,          # Maximum depth of a tree\n",
        "        random_state=42       # For reproducibility\n",
        "        # You can add more hyperparameters here based on your specific problem and data\n",
        "    )\n",
        "\n",
        "    # Train the XGBoost model on the original data\n",
        "    model.fit(X_train, train_df[metric])\n",
        "\n",
        "    # Add the model to the models dictionary\n",
        "    models[metric] = model\n",
        "\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAcHyg2S3UBi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import joblib\n",
        "\n",
        "# Initialize a dictionary to store loaded models\n",
        "loaded_models = {}\n",
        "\n",
        "# List of metrics for which you want to load models\n",
        "metrics = ['attackVector']\n",
        "\n",
        "# Load the trained models\n",
        "for metric in metrics:\n",
        "    model_filename = f\"/content/drive/My Drive/trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    loaded_model = joblib.load(model_filename)\n",
        "    loaded_models[metric] = loaded_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWSS-GgW3URp",
        "outputId": "adce32c6-7f35-40d2-c87e-37665009b536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance metrics for attackVector:\n",
            "Accuracy: 0.9183863885839737\n",
            "Precision: 0.9227500696572861\n",
            "Recall: 0.972256880733945\n",
            "F1-score: 0.9468567956827848\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the predictions dictionary with empty lists for each metric\n",
        "predictions = {\n",
        "    'attackVector': []\n",
        "\n",
        "}\n",
        "\n",
        "# Make predictions on the test set for each metric\n",
        "for metric, model in models.items():\n",
        "    predictions[metric] = model.predict(X_test)\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predictions[metric])\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predictions[metric], output_dict=True)\n",
        "    precision = report[str(1)]['precision']\n",
        "    recall = report[str(1)]['recall']\n",
        "    f1_score = report[str(1)]['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model, including the base severity\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBVN4TNPtUwF",
        "outputId": "d2edcc1e-21de-4079-c13c-96e6f342acb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model for metric 'attackComplexity' saved as '/content/drive/My Drive/trained_models/XGBOOST/attackComplexity_XGBoost_tfidf_labelEncoding.pkl'\n",
            "Time taken: 296.6600925922394 seconds\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics = ['attackComplexity']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Define the dictionary to store XGBoost models\n",
        "models = {}\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through each metric and train an XGBoost model without SMOTE\n",
        "for metric in metrics:\n",
        "    # Create the XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=0.1,       # Learning rate for boosting\n",
        "    n_estimators=500,        # Number of boosting rounds\n",
        "    max_depth=5,             # Maximum depth of a tree\n",
        "\n",
        "    objective='binary:logistic',  # Learning task and objective function\n",
        "    eval_metric='logloss',   # Evaluation metric\n",
        "    random_state=42\n",
        "        # You can add more hyperparameters here based on your specific problem and data\n",
        "    )\n",
        "\n",
        "    # Train the XGBoost model on the original data\n",
        "    model.fit(X_train, train_df[metric])\n",
        "\n",
        "    # Add the model to the models dictionary\n",
        "    models[metric] = model\n",
        "\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpbHDFrDBbpZ",
        "outputId": "399e5e34-d74a-437e-d34b-14e813f10f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance metrics for attackComplexity:\n",
            "Accuracy: 0.9373765093304062\n",
            "Precision: 0.9401925425269076\n",
            "Recall: 0.9947120151428658\n",
            "F1-score: 0.9666841859378651\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the predictions dictionary with empty lists for each metric\n",
        "predictions = {\n",
        "    'attackComplexity': []\n",
        "\n",
        "}\n",
        "\n",
        "# Make predictions on the test set for each metric\n",
        "for metric, model in models.items():\n",
        "    predictions[metric] = model.predict(X_test)\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predictions[metric])\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predictions[metric], output_dict=True)\n",
        "    precision = report[str(1)]['precision']\n",
        "    recall = report[str(1)]['recall']\n",
        "    f1_score = report[str(1)]['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model, including the base severity\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeaJ0JcAQyFe",
        "outputId": "46e5c1d4-4279-498e-aed0-0ce1a27e2644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model for metric 'attackComplexity' saved as '/content/drive/My Drive/trained_models/XGBOOST/attackComplexity_XGBoost_tfidf_labelEncoding.pkl'\n",
            "Time taken: 336.361013174057 seconds\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics = ['attackComplexity']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Define the dictionary to store XGBoost models\n",
        "models = {}\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through each metric and train an XGBoost model without SMOTE\n",
        "for metric in metrics:\n",
        "    # Create the XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=0.5,       # Learning rate for boosting\n",
        "    n_estimators=300,        # Number of boosting rounds\n",
        "    max_depth=5,             # Maximum depth of a tree\n",
        "\n",
        "    random_state=42\n",
        "        # You can add more hyperparameters here based on your specific problem and data\n",
        "    )\n",
        "\n",
        "    # Train the XGBoost model on the original data\n",
        "    model.fit(X_train, train_df[metric])\n",
        "\n",
        "    # Add the model to the models dictionary\n",
        "    models[metric] = model\n",
        "\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krxZ_NX2QyR7",
        "outputId": "cd7b5da5-97da-40e9-ef59-67e4a2222d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance metrics for attackComplexity:\n",
            "Accuracy: 0.9339462129527991\n",
            "Precision: 0.946443030653557\n",
            "Recall: 0.9833248204789232\n",
            "F1-score: 0.9645314825457172\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the predictions dictionary with empty lists for each metric\n",
        "predictions = {\n",
        "    'attackComplexity': []\n",
        "\n",
        "}\n",
        "\n",
        "# Make predictions on the test set for each metric\n",
        "for metric, model in models.items():\n",
        "    predictions[metric] = model.predict(X_test)\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predictions[metric])\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predictions[metric], output_dict=True)\n",
        "    precision = report[str(1)]['precision']\n",
        "    recall = report[str(1)]['recall']\n",
        "    f1_score = report[str(1)]['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model, including the base severity\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKStzlktQybY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j170q9XJQyk8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9iRuvoCQyuo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLnU9xwvQy7F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8Rd9w00tU15",
        "outputId": "e230791a-c212-40c0-b91f-a4721ab005d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model for metric 'privilegesRequired' saved as '/content/drive/My Drive/trained_models/XGBOOST/privilegesRequired_XGBoost_tfidf_labelEncoding.pkl'\n",
            "Time taken: 854.6107122898102 seconds\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics = ['privilegesRequired']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Define the dictionary to store XGBoost models\n",
        "models = {}\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through each metric and train an XGBoost model without SMOTE\n",
        "for metric in metrics:\n",
        "    # Create the XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=0.3,    # Learning rate for boosting\n",
        "        n_estimators=500,     # Number of boosting rounds\n",
        "        max_depth=5,          # Maximum depth of a tree\n",
        "        random_state=42       # For reproducibility\n",
        "        # You can add more hyperparameters here based on your specific problem and data\n",
        "    )\n",
        "\n",
        "    # Train the XGBoost model on the original data\n",
        "    model.fit(X_train, train_df[metric])\n",
        "\n",
        "    # Add the model to the models dictionary\n",
        "    models[metric] = model\n",
        "\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9uR9ZU9tU4n",
        "outputId": "1cc251ef-4e44-4bec-d152-140d1917c3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance metrics for privilegesRequired:\n",
            "Accuracy: 0.8241218441273326\n",
            "Precision: 0.7853830867741223\n",
            "Recall: 0.672782296198222\n",
            "F1-score: 0.7247351263243683\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the predictions dictionary with empty lists for each metric\n",
        "predictions = {\n",
        "    'privilegesRequired': []\n",
        "\n",
        "}\n",
        "\n",
        "# Make predictions on the test set for each metric\n",
        "for metric, model in models.items():\n",
        "    predictions[metric] = model.predict(X_test)\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predictions[metric])\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predictions[metric], output_dict=True)\n",
        "    precision = report[str(1)]['precision']\n",
        "    recall = report[str(1)]['recall']\n",
        "    f1_score = report[str(1)]['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model, including the base severity\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M24h6wLXtU7g",
        "outputId": "945c44d5-e8f7-41a9-991f-b9cf0a9af678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model for metric 'userInteraction' saved as '/content/drive/My Drive/trained_models/XGBOOST/userInteraction_XGBoost_tfidf_labelEncoding.pkl'\n",
            "Time taken: 166.7086112499237 seconds\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics = ['userInteraction']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Define the dictionary to store XGBoost models\n",
        "models = {}\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through each metric and train an XGBoost model without SMOTE\n",
        "for metric in metrics:\n",
        "    # Create the XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=0.5,    # Learning rate for boosting\n",
        "        n_estimators=300,     # Number of boosting rounds\n",
        "        max_depth=5,          # Maximum depth of a tree\n",
        "        random_state=42       # For reproducibility\n",
        "        # You can add more hyperparameters here based on your specific problem and data\n",
        "    )\n",
        "\n",
        "    # Train the XGBoost model on the original data\n",
        "    model.fit(X_train, train_df[metric])\n",
        "\n",
        "    # Add the model to the models dictionary\n",
        "    models[metric] = model\n",
        "\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MwTcq2LtU_a",
        "outputId": "3824e99e-4bcf-4470-9672-b00f31341d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance metrics for userInteraction:\n",
            "Accuracy: 0.9169593852908892\n",
            "Precision: 0.9183937823834197\n",
            "Recall: 0.848974704385571\n",
            "F1-score: 0.8823209146768297\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the predictions dictionary with empty lists for each metric\n",
        "predictions = {\n",
        "    'userInteraction': []\n",
        "\n",
        "}\n",
        "\n",
        "# Make predictions on the test set for each metric\n",
        "for metric, model in models.items():\n",
        "    predictions[metric] = model.predict(X_test)\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predictions[metric])\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predictions[metric], output_dict=True)\n",
        "    precision = report[str(1)]['precision']\n",
        "    recall = report[str(1)]['recall']\n",
        "    f1_score = report[str(1)]['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model, including the base severity\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A5wkM-qtVC_",
        "outputId": "ef19cdf2-caa4-40d2-8541-b64128a08c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model for metric 'scope' saved as '/content/drive/My Drive/trained_models/XGBOOST/scope_XGBoost_tfidf_labelEncoding.pkl'\n",
            "Time taken: 355.76378297805786 seconds\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics = ['scope']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Define the dictionary to store XGBoost models\n",
        "models = {}\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through each metric and train an XGBoost model without SMOTE\n",
        "for metric in metrics:\n",
        "    # Create the XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=0.6,    # Learning rate for boosting\n",
        "        n_estimators=600,     # Number of boosting rounds\n",
        "        max_depth=6,          # Maximum depth of a tree\n",
        "        random_state=42       # For reproducibility\n",
        "        # You can add more hyperparameters here based on your specific problem and data\n",
        "    )\n",
        "\n",
        "    # Train the XGBoost model on the original data\n",
        "    model.fit(X_train, train_df[metric])\n",
        "\n",
        "    # Add the model to the models dictionary\n",
        "    models[metric] = model\n",
        "\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QilMtIxKVyz",
        "outputId": "95273c5d-be61-411a-fa47-8f317cabf833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance metrics for scope:\n",
            "Accuracy: 0.9319978046103183\n",
            "Precision: 0.9475379659456972\n",
            "Recall: 0.970311027332705\n",
            "F1-score: 0.9587892898719441\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the predictions dictionary with empty lists for each metric\n",
        "predictions = {\n",
        "    'scope': []\n",
        "\n",
        "}\n",
        "\n",
        "# Make predictions on the test set for each metric\n",
        "for metric, model in models.items():\n",
        "    predictions[metric] = model.predict(X_test)\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predictions[metric])\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predictions[metric], output_dict=True)\n",
        "    precision = report[str(1)]['precision']\n",
        "    recall = report[str(1)]['recall']\n",
        "    f1_score = report[str(1)]['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model, including the base severity\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tomFF-L8G7Wo",
        "outputId": "7d570d66-fd8d-47e3-b4a7-56edcc449552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model for metric 'scope' saved as '/content/drive/My Drive/trained_models/XGBOOST/scope_XGBoost_tfidf_labelEncoding.pkl'\n",
            "Time taken: 320.1301667690277 seconds\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics = ['scope']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Define the dictionary to store XGBoost models\n",
        "models = {}\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through each metric and train an XGBoost model without SMOTE\n",
        "for metric in metrics:\n",
        "    # Create the XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=0.1,    # Learning rate for boosting\n",
        "        n_estimators=600,     # Number of boosting rounds\n",
        "        max_depth=5,          # Maximum depth of a tree\n",
        "        random_state=42       # For reproducibility\n",
        "        # You can add more hyperparameters here based on your specific problem and data\n",
        "    )\n",
        "\n",
        "    # Train the XGBoost model on the original data\n",
        "    model.fit(X_train, train_df[metric])\n",
        "\n",
        "    # Add the model to the models dictionary\n",
        "    models[metric] = model\n",
        "\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2EiuWTBHH-r",
        "outputId": "5e522212-ff75-471e-8fd4-86b9aa9c7176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance metrics for scope:\n",
            "Accuracy: 0.9403951701427004\n",
            "Precision: 0.9447603049489598\n",
            "Recall: 0.9844486333647502\n",
            "F1-score: 0.9641962284056442\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the predictions dictionary with empty lists for each metric\n",
        "predictions = {\n",
        "    'scope': []\n",
        "\n",
        "}\n",
        "\n",
        "# Make predictions on the test set for each metric\n",
        "for metric, model in models.items():\n",
        "    predictions[metric] = model.predict(X_test)\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predictions[metric])\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predictions[metric], output_dict=True)\n",
        "    precision = report[str(1)]['precision']\n",
        "    recall = report[str(1)]['recall']\n",
        "    f1_score = report[str(1)]['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model, including the base severity\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aV6OzWmK7gS",
        "outputId": "a771acd0-1389-44f9-b2cf-621a73494d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model for metric 'confidentialityImpact' saved as '/content/drive/My Drive/trained_models/XGBOOST/confidentialityImpact_XGBoost_tfidf_labelEncoding.pkl'\n",
            "Time taken: 982.2845175266266 seconds\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics = ['confidentialityImpact']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Define the dictionary to store XGBoost models\n",
        "models = {}\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through each metric and train an XGBoost model without SMOTE\n",
        "for metric in metrics:\n",
        "    # Create the XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=0.1,    # Learning rate for boosting\n",
        "        n_estimators=500,     # Number of boosting rounds\n",
        "        max_depth=5,          # Maximum depth of a tree\n",
        "        random_state=42       # For reproducibility\n",
        "        # You can add more hyperparameters here based on your specific problem and data\n",
        "    )\n",
        "\n",
        "    # Train the XGBoost model on the original data\n",
        "    model.fit(X_train, train_df[metric])\n",
        "\n",
        "    # Add the model to the models dictionary\n",
        "    models[metric] = model\n",
        "\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kgO8_opKK7jl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7d212b-e4f5-457e-8d62-c841cb249ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance metrics for confidentialityImpact:\n",
            "Accuracy: 0.8405598243688255\n",
            "Precision: 0.8790832905763639\n",
            "Recall: 0.7075017205781142\n",
            "F1-score: 0.7840146430750456\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the predictions dictionary with empty lists for each metric\n",
        "predictions = {\n",
        "    'confidentialityImpact': []\n",
        "\n",
        "}\n",
        "\n",
        "# Make predictions on the test set for each metric\n",
        "for metric, model in models.items():\n",
        "    predictions[metric] = model.predict(X_test)\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predictions[metric])\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predictions[metric], output_dict=True)\n",
        "    precision = report[str(1)]['precision']\n",
        "    recall = report[str(1)]['recall']\n",
        "    f1_score = report[str(1)]['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model, including the base severity\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VmZ7Me7bK7m3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ad8112-f6d6-4fb1-fd3a-06309803b7bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model for metric 'integrityImpact' saved as '/content/drive/My Drive/trained_models/XGBOOST/integrityImpact_XGBoost_tfidf_labelEncoding.pkl'\n",
            "Time taken: 1162.2717826366425 seconds\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics = ['integrityImpact']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Define the dictionary to store XGBoost models\n",
        "models = {}\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through each metric and train an XGBoost model without SMOTE\n",
        "for metric in metrics:\n",
        "    # Create the XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=0.1,    # Learning rate for boosting\n",
        "        n_estimators=600,     # Number of boosting rounds\n",
        "        max_depth=5,          # Maximum depth of a tree\n",
        "        random_state=42       # For reproducibility\n",
        "        # You can add more hyperparameters here based on your specific problem and data\n",
        "    )\n",
        "\n",
        "    # Train the XGBoost model on the original data\n",
        "    model.fit(X_train, train_df[metric])\n",
        "\n",
        "    # Add the model to the models dictionary\n",
        "    models[metric] = model\n",
        "\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Hzg-2f52K72f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83689762-c935-40e9-87c0-879e950626d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance metrics for integrityImpact:\n",
            "Accuracy: 0.8582601536772777\n",
            "Precision: 0.9104633003643935\n",
            "Recall: 0.7753805231269396\n",
            "F1-score: 0.8375099760574621\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the predictions dictionary with empty lists for each metric\n",
        "predictions = {\n",
        "    'integrityImpact': []\n",
        "\n",
        "}\n",
        "\n",
        "# Make predictions on the test set for each metric\n",
        "for metric, model in models.items():\n",
        "    predictions[metric] = model.predict(X_test)\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predictions[metric])\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predictions[metric], output_dict=True)\n",
        "    precision = report[str(1)]['precision']\n",
        "    recall = report[str(1)]['recall']\n",
        "    f1_score = report[str(1)]['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model, including the base severity\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "95rFwj8BLPRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1b4e9e-a03c-47d2-ed54-3e6b0cf725db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model for metric 'availabilityImpact' saved as '/content/drive/My Drive/trained_models/XGBOOST/availabilityImpact_XGBoost_tfidf_labelEncoding.pkl'\n",
            "Time taken: 172.3548607826233 seconds\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "metrics = ['availabilityImpact']\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for metric in metrics:\n",
        "    train_df[metric] = label_encoder.fit_transform(train_df[metric])\n",
        "    test_df[metric] = label_encoder.transform(test_df[metric])\n",
        "\n",
        "# Define the path to your Google Drive folder\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Define the dictionary to store XGBoost models\n",
        "models = {}\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Loop through each metric and train an XGBoost model without SMOTE\n",
        "for metric in metrics:\n",
        "    # Create the XGBoost model\n",
        "    model = xgb.XGBClassifier(\n",
        "        learning_rate=0.5,    # Learning rate for boosting\n",
        "        n_estimators=300,     # Number of boosting rounds\n",
        "        max_depth=5,          # Maximum depth of a tree\n",
        "        random_state=42       # For reproducibility\n",
        "        # You can add more hyperparameters here based on your specific problem and data\n",
        "    )\n",
        "\n",
        "    # Train the XGBoost model on the original data\n",
        "    model.fit(X_train, train_df[metric])\n",
        "\n",
        "    # Add the model to the models dictionary\n",
        "    models[metric] = model\n",
        "\n",
        "    # Update the model saving path to your Google Drive folder\n",
        "    model_filename = f\"{drive_path}trained_models/XGBOOST/{metric}_XGBoost_tfidf_labelEncoding.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"Model for metric '{metric}' saved as '{model_filename}'\")\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken:\", elapsed_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qxmB-g3vLPU4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47266bf9-8f70-406e-d885-e43941779faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance metrics for availabilityImpact:\n",
            "Accuracy: 0.8986004390779363\n",
            "Precision: 0.8756156826689433\n",
            "Recall: 0.8809964318752478\n",
            "F1-score: 0.8782978162774612\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the predictions dictionary with empty lists for each metric\n",
        "predictions = {\n",
        "    'availabilityImpact': []\n",
        "\n",
        "}\n",
        "\n",
        "# Make predictions on the test set for each metric\n",
        "for metric, model in models.items():\n",
        "    predictions[metric] = model.predict(X_test)\n",
        "\n",
        "# Initialize a dictionary to store the performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "# Iterate through each metric and its corresponding model\n",
        "for metric, model in models.items():\n",
        "    # Calculate the accuracy score for the current model\n",
        "    accuracy = accuracy_score(test_df[metric], predictions[metric])\n",
        "\n",
        "    # Calculate other performance metrics (precision, recall, F1-score) using classification_report\n",
        "    report = classification_report(test_df[metric], predictions[metric], output_dict=True)\n",
        "    precision = report[str(1)]['precision']\n",
        "    recall = report[str(1)]['recall']\n",
        "    f1_score = report[str(1)]['f1-score']\n",
        "\n",
        "    # Store the performance metrics in the performance_metrics dictionary\n",
        "    performance_metrics[metric] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }\n",
        "\n",
        "# Print the performance metrics for each model, including the base severity\n",
        "for metric, metrics_dict in performance_metrics.items():\n",
        "    print(f\"Performance metrics for {metric}:\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']}\")\n",
        "    print(f\"F1-score: {metrics_dict['f1_score']}\")\n",
        "    print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTixiqrusT8gdraQET6omk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}